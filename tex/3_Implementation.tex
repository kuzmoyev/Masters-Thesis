\chapter{Implementation}\label{ch:implementation}

This chapter provides details of the implementation, used tools, training and testing of the models.


\section{Used tools}\label{sec:used-tools}

\textit{Python} was used as a primary programming language for implementation of the project. Input streams are
processed by \textit{pyaudio} library. Data is stored in a \textit{numpy} array in 16 bit integers. \textit{Numpy} and
\textit{Pandas} were used for datasets processing. \textit{Scipy}'s implementation of \acp{DNN} (and other models that
were tested) was used for models of event detection module. Models were trained in \textit{jupyter notebook} included
in the implementation sources. Models are serialized into .pickle format using Python's \textit{joblib} module.
The \textit{statsmodels} library is used to generate \ac{ARIMA} model used in time signature estimation

As was discussed in \cref{ch:analysis-and-design}, implementations of \textit{Spleeter} and \textit{Madmom} were used
for source splitting and tempo detection modules respectively. Output score in lilypond format is generated using
\textit{abjad} that has Pythonic object-oriented interface for sheet music engraving which uses the lilypond compiler.



\section{Project structure}\label{sec:project-structure}
Project follows common approaches for Python project structures and implements a Python module as well as \ac{CLI} for
music transcription. It is well parametrized such that user can define a set and tuning of the instruments, tempo, time
signature and key. Otherwise these parameters are being estimated by dedicated modules according to design described in
a \cref{ch:analysis-and-design}.

The main endpoint that is used by \ac{CLI} and may be used by developers is \textit{mimt.music\_transcription} (mimt
stands for multi-instrument music transcription). It is responsible for the whole data pipeline that starts at input
stream reading.

\subsection{Input stream}\label{subsec:input-stream}
Input stream can have any source: file, microphone, or any other source which implements
\textit{audio\_reading.\_AbstractStream} interface. For microphone and file streaming there are already implemented
classes \textit{MicrophoneStream} and \textit{FileStream} respectively. Implementations are parametrized by size of read
chunks, but it is important to note that divergence from default value will affect the representation of the data which
in its turn would require retraining of the models used in event detection.


\subsection{Music source separation and tempo estimation}\label{subsec:music-source-separation-and-tempo-estimation}
Modules of the implementation utilize existing solutions for problems of source separation (\textit{Spleeter}) and tempo
estimation (\textit{Madmom}). There are a dedicated modules for both of the problems. The modules are just calling
needed functions from implementations of \textit{Spleeter} and \textit{Madmom}. Hence, modules are simple and were
included into a project as a dedicated Python modules only to be consistent within a structure and interfaces of
the implementation.

By default, this implementation uses five stems separation. Otherwise it can be defined as a parameter of the analysis.
If any of the output sheet music scores does not have any notes, it is ignored and does not have output sheet music.

\subsection{Pitch detection}\label{subsec:pitch-detection}
Following the source separation module, pitch extraction transforms signals of each instrument from time domain to
a frequency domain using \ac{FFT}. Numpy's \textit{fft} submodule is used to perform the transformation. Amplitudes
of frequencies generated by \ac{FFT} then converted to decibels with the following formula:
\[volume = 10 * \log_{10}amplitude\]
\textit{PitchExtractor} stores the frame rate of the input stream to correctly convert frequencies produced by \ac{FFT}
to real frequencies of a sound (Hz):
\[frequency = fft\_frequency * frame\_rate\]

Frequencies are then converted to the closest notes. Assuming equal temperament tuning system and note A tuned to
440 Hz, each frequency is converted to a note in a format used in \textit{abjad} modules, specifically
\textit{\{note\_name\}\{octave\}}, e.g. \textit{A2} is 440 Hz \textit{A} note, \textit{C2} is the \textit{middle C},
\textit{Cs3} is a \textit{C$\sharp$} - one octave and a semitone higher of the \textit{middle C}, etc. First,
the semitone number above or bellow \textit{440-Hz-A} is calculated:
\[n = \lfloor \log_{12}{\frac{frequency}{440}} \rceil\]
where $n=0$ for the \textit{440-Hz-A}, $n=1$ for \textit{A$\sharp$}, $n=-9$ for the \textit{middle C}, etc.

The \textit{note\_name} and \textit{octave} are then calculated simply:
\[note\_name = NOTES[n \bmod 12]\]
where
\[NOTES = [A, As, B, C, Cs, D, Ds, E, F, Fs, G, Gs]\]
and
\[octave = 2 + \lfloor \frac{n}{12} \rfloor\]
where $2$ is an octave of the \textit{440-Hz-A}.

Only the maximum volume value is selected from the frequencies that are converted to the same note.

\subsection{Event detection}\label{subsec:event-detection}
\textit{EventDetector} from \textit{event\_detection} module goes through the spectrogram (time-volume representation of
the notes) of the sound with overlapping window as shown in \cref{fig:pitch-detection} (green). The size of the window
is determined by 3 parameters: shortest note value (default is 16(\Sech)), sampling rate of input stream, and
overlapping rate which defines how many data points (volume in time) are shared between two consequent windows. Having
these 3 parameters, the size of the window is calculated as
\[window\_size = sampling\_rate / (shortest\_note\_value / overlapping\_rate)\]
and step between windows as
\[window\_step = \lfloor{sampling\_rate / shortest\_note\_value}\rfloor\]

The window of spectrogram is passed to the pretrained model for the given instrument to detect whether this window
contains the envelope(s) for the note(s) in this point in time. If it does, this window constitutes the start of
the note. As was mention in \cref{sec:event-detection}, the end of the detected note is a point in time when its volume
drops under specified threshold $note\_volume-10$.

